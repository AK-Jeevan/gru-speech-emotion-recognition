# ğŸ§ Speech Emotion Recognition using GRU + MFCC

This project implements a **GRU-based deep learning pipeline** to classify speech recordings into emotional categories based on MFCC features.

It supports multiple speakers and emotions, automatically extracts MFCCs, pads variable-length sequences, and trains a GRU classifier with early stopping + checkpointing.

---

## âœ¨ Features
âœ… Automatic MFCC extraction from audio  
âœ… Multi-speaker support  
âœ… GRU-based RNN architecture  
âœ… Stratified train/val/test split  
âœ… Emotion label mapping  
âœ… EarlyStopping + ModelCheckpoint  
âœ… MFCC visualization per sample  

---

## ğŸš Emotion Classes
- neutral  
- calm  
- happy  
- sad  
- angry  
- fearful  
- disgust  
- surprised  

---

## ğŸ“‚ Folder Structure
Place audio files inside:
data/Voice_To_Speech/
â”‚â”€â”€ Actor_01/
â”‚â”€â”€ Actor_02/
â”‚â”€â”€ ...

File names must include emotion code mapping ("01" â†’ neutral, etc.).

---

## ğŸ§  Model Architecture
Masking
GRU(128, return_sequences=True)
Dropout(0.3)
GRU(64)
Dropout(0.3)
Dense(64, relu)
Dropout(0.3)
Dense(n_classes, softmax)

---

## ğŸ“¦ Installation

git clone https://github.com/<your-username>/gru-speech-emotion-recognition
cd gru-speech-emotion-recognition
pip install -r requirements.txt

## â–¶ï¸ Training

python train.py

## ğŸ” Evaluation

Test accuracy + loss

Prediction samples printed

MFCC visualizations saved automatically

## âœ… Output

Best model: best_gru_ser_allactors.h5

Final model: gru_ser_allactors_model.h5

Label encoder: label_encoder_allactors.pkl

MFCC visualizations: mfcc_visualizations/

## ğŸ’¡ Improvements

Add CNN + RNN hybrid

Add attention mechanism

Use spectrograms or mel-spectrograms

Hyperparameter tuning

Real-time voice inference

## ğŸ“„ License
MIT
